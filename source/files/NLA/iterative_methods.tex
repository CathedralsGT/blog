\documentclass[12pt]{amsart}
\usepackage{amsmath, amsthm, amssymb, bm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]

\title{Iterative Methods: An Introduction}
% \author{}
% \address{}
% \email{}

\begin{document}
\maketitle

\begin{abstract}
    In this note we will discuss the definition of iterative methods, convergence theory, error estimation, analysis of convergence speed, as well as some
    basic properties which we will frequently encountered in the following sections.
    \end{abstract}

\section{Introduction}
Different from direct methods, which compute the exact solution to a Linear system({\small{\itshape if we ignore the roundoff error, yes}}) after a finite number 
of steps, iterative methods approach the exact solution $\bm{X}^*$ by generating an infinite sequence of vectors $\{\bm{X}_k \}_{k=1}^\infty $, whose error decreases 
as the iteration continues($\lim_{k \to \infty}\bm{X}_k = \bm{X}^* $). In practice, we typically set an error threshold and stop the iteration once the estimated error  
falls below it. This motivates the design of iterative methods that converges rapidly, which will be covered in the next note.

\section{General Iterative Methods}
\begin{definition}[Iterative Methods]
    Suppose we have a linear equation \[A\bm{X}=\bm{b}\] and $A$ can be decomposed by $A=A_1-A_2$, where $A_1$ is an invertible 
    matrix. Then we get another linear equation \[A_1\bm{X}=A_2\bm{X}+\bm{b}\] which has the same solution set with the initial equation.
    By solving the induced equation we get \[\bm{X}^*=A_1^{-1}A_2\bm{X}^*+A_1^{-1}\bm{b}\]
    define $M:=A_1^{-1}A_2$, $\bm{g}:=A_1^{-1}\bm{b}$. Then $\forall \bm{X}_0 \in \mathbb{R}^n$,\footnote{Here $\bm{X}_0$ is an arbitrary guess of the solution}  the following iteration can be used to approximate the solution:
    \[\bm{X}_k=M\bm{X}_{k-1}+\bm{g}\]

\end{definition}

\begin{remark}
    This iteration scheme $\bm{X}_k=M\bm{X}_{k-1}+\bm{g}$ is known as a \textbf{stationary iterative method}\footnote{In fact, many useful iteration methods are stationary, including Jacobi's method, Gauss-Seidel method, Successive Overrelaxation, etc. So for simplicity we will only focus on analysing the properties of stationary iterative methods.}, 
    which means that the iteration matrix $M$ and iteration vector $\bm{g}$ remain fixed throughout the entire process. More generally, 
    an iterative method can be represented in the form
    \[\bm{X}_k=f_k(\bm{X}_{k-r+1},\dots,\bm{X}_{k-1})\] where we have a sequence of function ${f_k}$ that may vary with $k$, indicating that 
    the method is nonstationary. 
\end{remark}

\section{the convergence theory of iteration methods}
\begin{definition}[Convergence of an iteration method]
    Suppose $A\bm{X}=\bm{b}$ is equivalent to the fixed-point formulation $\bm{X}=M\bm{X}+\bm{g}$. If $\forall \bm{X} \in \mathbb{R}^n$, 
    the sequence $\{\bm{X}_k\}_{k=0}^\infty$ generated by the iteration \[\bm{X}_k=M\bm{X}_{k-1}+\bm{g}\] has a limit $\bm{X}^*=\lim_{k \to \infty}\bm{X}_k$
    then the iteration method is said to be convergent. Moreover, $\bm{X}^*$ is necessarily the unique solution to the original equation.
\end{definition}

Now we present a necessary and sufficient condition for the convergence of the iteration.

\begin{theorem}
    The iteration method $\bm{X}_k=M\bm{X}_{k-1}+\bm{g}$ is convergent iff $\rho(M)<1$
\end{theorem}
\begin{proof}[sketch of proof]
    We notice that the exact solution $\bm{X}^*$ satisfies \[\bm{X}^*=M\bm{X}^*+\bm{g}\] then subtracting $\bm{X}_{k+1}$ from $\bm{X}^*$ we get
    \[
    \|\bm{X}^*-\bm{X}_{k+1}\| = \|M\bm{X}^*+\bm{g}-M\bm{X}_k-\bm{g}\| = \|M(\bm{X}^*-\bm{X}_k)\|, \forall \bm{X}_0 \in \mathbb{R}^n
    \]
    Using induction we can get $\|\bm{X}^*-\bm{X}_{k+1}\|\leq\|M^k\|\|\bm{X}^*-\bm{X}_0\|$, since M is a convergent matrix by $\rho(M)\leq1$, RHS will
    tend to 0 as $k\to\infty$, yielding the result.
\end{proof}






















\end{document}
